#!/usr/bin/env python3
"""
Improved Model Trainer - ä½¿ç”¨æ¸…ç†åçš„æ•°æ®å’Œç”Ÿç‰©å­¦çº¦æŸè®­ç»ƒæ”¹è¿›æ¨¡å‹
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

class ImprovedBacteriaGrowthModel:
    def __init__(self):
        self.model = None
        self.genus_encoder = LabelEncoder()
        self.species_encoder = LabelEncoder()
        self.scaler = StandardScaler()
        
    def biological_continuity_loss(self, y_true, y_pred):
        """ç”Ÿç‰©å­¦è¿ç»­æ€§æŸå¤±å‡½æ•° - æƒ©ç½šç›¸é‚»æ—¶é—´ç‚¹çš„å‰§çƒˆå˜åŒ–"""

        # åŸºç¡€MSEæŸå¤±
        mse_loss = tf.reduce_mean(tf.square(y_true - y_pred))

        # è¿ç»­æ€§æŸå¤± - æƒ©ç½šç›¸é‚»æ—¶é—´ç‚¹çš„å‰§çƒˆå˜åŒ–
        diff_true = y_true[:, 1:] - y_true[:, :-1]
        diff_pred = y_pred[:, 1:] - y_pred[:, :-1]
        continuity_loss = tf.reduce_mean(tf.square(diff_true - diff_pred))

        # å•è°ƒæ€§æŸå¤± - åœ¨æ—©æœŸé˜¶æ®µï¼ˆå‰8å°æ—¶ï¼‰æƒ©ç½šå‰§çƒˆä¸‹é™
        early_pred = y_pred[:, :8]  # å‰8ä¸ªæ—¶é—´ç‚¹
        early_diff = early_pred[:, 1:] - early_pred[:, :-1]
        # æƒ©ç½šè´Ÿçš„å˜åŒ–ï¼ˆä¸‹é™ï¼‰
        monotonic_penalty = tf.reduce_mean(tf.maximum(-early_diff, 0.0))

        # é›¶å€¼æƒ©ç½š - å¼ºçƒˆæƒ©ç½šé¢„æµ‹ä¸ºé›¶æˆ–æå°å€¼
        zero_penalty = tf.reduce_mean(tf.maximum(3.0 - y_pred, 0.0))  # log10(1000) â‰ˆ 3.0

        # ç»„åˆæŸå¤±
        total_loss = mse_loss + 0.1 * continuity_loss + 0.2 * monotonic_penalty + 0.5 * zero_penalty

        return total_loss
    
    def create_improved_model(self, num_genera, num_species):
        """åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹æ¶æ„"""
        
        # è¾“å…¥å±‚
        genus_input = Input(shape=(1,), name='genus_input')
        species_input = Input(shape=(1,), name='species_input')
        env_input = Input(shape=(2,), name='env_input')  # pH, temperature
        
        # åµŒå…¥å±‚ - æ›´å¤§çš„åµŒå…¥ç»´åº¦
        genus_embedding = Embedding(num_genera, 16, name='genus_embedding')(genus_input)
        species_embedding = Embedding(num_species, 16, name='species_embedding')(species_input)
        
        # å±•å¹³åµŒå…¥
        genus_flat = Flatten()(genus_embedding)
        species_flat = Flatten()(species_embedding)
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        merged = Concatenate()([genus_flat, species_flat, env_input])
        
        # æ›´æ·±çš„ç½‘ç»œç»“æ„ï¼ŒåŠ å…¥æ‰¹å½’ä¸€åŒ–å’Œdropout
        x = Dense(256, activation='relu')(merged)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)
        
        x = Dense(512, activation='relu')(x)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)
        
        x = Dense(256, activation='relu')(x)
        x = BatchNormalization()(x)
        x = Dropout(0.2)(x)
        
        x = Dense(128, activation='relu')(x)
        x = BatchNormalization()(x)
        x = Dropout(0.2)(x)
        
        # è¾“å‡ºå±‚ - 24ä¸ªæ—¶é—´ç‚¹çš„å¯†åº¦é¢„æµ‹
        # ä½¿ç”¨sigmoidæ¿€æ´»ç¡®ä¿è¾“å‡ºåœ¨åˆç†èŒƒå›´å†…ï¼Œç„¶åç¼©æ”¾åˆ°log10èŒƒå›´
        output = Dense(24, activation='sigmoid', name='growth_curve')(x)
        # å°†sigmoidè¾“å‡º(0-1)æ˜ å°„åˆ°åˆç†çš„log10èŒƒå›´(3-9)ï¼Œå¯¹åº”1e3åˆ°1e9 CFU/mL
        output = tf.keras.layers.Lambda(lambda x: x * 6.0 + 3.0)(output)
        
        model = Model(inputs=[genus_input, species_input, env_input], outputs=output)
        
        return model
    
    def prepare_data(self, csv_file):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        
        print(f"ğŸ“‚ åŠ è½½æ•°æ®: {csv_file}")
        df = pd.read_csv(csv_file)
        
        # è§£æç»†èŒåç§°
        bacteria_names = df.iloc[:, 0].str.split(' ', expand=True)
        genera = bacteria_names[0]
        species = bacteria_names[1]
        
        # ç¼–ç ç»†èŒä¿¡æ¯
        genus_encoded = self.genus_encoder.fit_transform(genera)
        species_encoded = self.species_encoder.fit_transform(species)
        
        # ç¯å¢ƒç‰¹å¾
        env_features = df[['ph', 'temperature']].values
        env_features_scaled = self.scaler.fit_transform(env_features)
        
        # å¯†åº¦æ•°æ® - è½¬æ¢ä¸ºlog10
        density_data = df.iloc[:, 3:27].values  # 24ä¸ªæ—¶é—´ç‚¹
        # ç¡®ä¿æ²¡æœ‰é›¶å€¼æˆ–è´Ÿå€¼
        density_data = np.maximum(density_data, 1.0)
        density_log = np.log10(density_data)
        
        print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   æ ·æœ¬æ•°: {len(df)}")
        print(f"   ç»†èŒå±: {len(self.genus_encoder.classes_)}")
        print(f"   ç»†èŒç§: {len(self.species_encoder.classes_)}")
        print(f"   å¯†åº¦èŒƒå›´: {np.min(density_data):.2e} - {np.max(density_data):.2e} CFU/mL")
        print(f"   Log10èŒƒå›´: {np.min(density_log):.2f} - {np.max(density_log):.2f}")
        
        return {
            'genus': genus_encoded,
            'species': species_encoded,
            'env': env_features_scaled,
            'density_log': density_log
        }
    
    def train_model(self, csv_file, epochs=100, batch_size=32):
        """è®­ç»ƒæ”¹è¿›çš„æ¨¡å‹"""
        
        # å‡†å¤‡æ•°æ®
        data = self.prepare_data(csv_file)
        
        # åˆ›å»ºæ¨¡å‹
        num_genera = len(self.genus_encoder.classes_)
        num_species = len(self.species_encoder.classes_)
        self.model = self.create_improved_model(num_genera, num_species)
        
        # ç¼–è¯‘æ¨¡å‹ - ä½¿ç”¨è‡ªå®šä¹‰æŸå¤±å‡½æ•°
        self.model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss=self.biological_continuity_loss,
            metrics=['mse']
        )
        
        print(f"\nğŸ—ï¸  æ¨¡å‹æ¶æ„:")
        self.model.summary()
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X = {
            'genus_input': data['genus'],
            'species_input': data['species'],
            'env_input': data['env']
        }
        y = data['density_log']
        
        # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯æ•°æ®
        X_train = {}
        X_val = {}
        for key in X.keys():
            X_train[key], X_val[key], y_train, y_val = train_test_split(
                X[key], y, test_size=0.2, random_state=42
            )
        
        print(f"\nğŸ“Š æ•°æ®åˆ†å‰²:")
        print(f"   è®­ç»ƒé›†: {len(y_train)} æ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(y_val)} æ ·æœ¬")
        
        # å›è°ƒå‡½æ•°
        callbacks = [
            EarlyStopping(patience=15, restore_best_weights=True, monitor='val_loss'),
            ReduceLROnPlateau(patience=8, factor=0.5, min_lr=1e-6, monitor='val_loss')
        ]
        
        # è®­ç»ƒæ¨¡å‹
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
        history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        return history
    
    def save_model(self, model_path):
        """ä¿å­˜æ¨¡å‹"""
        self.model.save(model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")

if __name__ == "__main__":
    # åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹è®­ç»ƒå™¨
    trainer = ImprovedBacteriaGrowthModel()
    
    # ä½¿ç”¨æ¸…ç†åçš„æ•°æ®è®­ç»ƒæ¨¡å‹
    print("ğŸ§  è®­ç»ƒæ”¹è¿›çš„ç»†èŒç”Ÿé•¿é¢„æµ‹æ¨¡å‹...")
    history = trainer.train_model("bacteria_24h_cleaned_dataset.csv", epochs=150, batch_size=64)
    
    # ä¿å­˜æ”¹è¿›çš„æ¨¡å‹
    trainer.save_model("bacteria_growth_model.h5")
    
    print("âœ… æ”¹è¿›æ¨¡å‹è®­ç»ƒå®Œæˆï¼")

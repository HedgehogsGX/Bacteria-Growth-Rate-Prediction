#!/usr/bin/env python3

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import json
import os

class ImprovedBacteriaGrowthModel:
    def __init__(self):
        self.model = None
        self.genus_encoder = LabelEncoder()
        self.species_encoder = LabelEncoder()
        self.scaler = StandardScaler()
        self.data_split_config = "data_split_config.json"
        
    def biological_continuity_loss(self, y_true, y_pred):
        mse_loss = tf.reduce_mean(tf.square(y_true - y_pred))
        diff_true = y_true[:, 1:] - y_true[:, :-1]
        diff_pred = y_pred[:, 1:] - y_pred[:, :-1]
        continuity_loss = tf.reduce_mean(tf.square(diff_true - diff_pred))
        early_pred = y_pred[:, :8]
        early_diff = early_pred[:, 1:] - early_pred[:, :-1]
        monotonic_penalty = tf.reduce_mean(tf.maximum(-early_diff, 0.0))
        zero_penalty = tf.reduce_mean(tf.maximum(3.0 - y_pred, 0.0))
        total_loss = mse_loss + 0.1 * continuity_loss + 0.2 * monotonic_penalty + 0.5 * zero_penalty
        return total_loss

    def create_data_split(self, data, train_size=0.6, val_size=0.2, test_size=0.2, random_state=42):
        print(f"ðŸ“Š Creating data split configuration...")
        print(f"   Train: {train_size*100:.0f}%, Validation: {val_size*100:.0f}%, Test: {test_size*100:.0f}%")
        assert abs(train_size + val_size + test_size - 1.0) < 1e-6, "Split sizes must sum to 1.0"

        total_samples = len(data['genus'])
        indices = np.arange(total_samples)

        # First split: separate test set (stratified by genus)
        train_val_indices, test_indices = train_test_split(
            indices,
            test_size=test_size,
            random_state=random_state,
            stratify=data['genus']
        )

        # Second split: separate train and validation from remaining data
        val_size_adjusted = val_size / (train_size + val_size)  # Adjust validation size
        train_indices, val_indices = train_test_split(
            train_val_indices,
            test_size=val_size_adjusted,
            random_state=random_state,
            stratify=data['genus'][train_val_indices]
        )

        # Create split configuration
        split_config = {
            'train_indices': train_indices.tolist(),
            'val_indices': val_indices.tolist(),
            'test_indices': test_indices.tolist(),
            'train_size': len(train_indices),
            'val_size': len(val_indices),
            'test_size': len(test_indices),
            'total_size': total_samples,
            'random_state': random_state,
            'created_timestamp': pd.Timestamp.now().isoformat()
        }

        # Save configuration
        with open(self.data_split_config, 'w') as f:
            json.dump(split_config, f, indent=2)

        print(f"âœ… Data split configuration saved to: {self.data_split_config}")
        print(f"   Training set: {len(train_indices)} samples ({len(train_indices)/total_samples*100:.1f}%)")
        print(f"   Validation set: {len(val_indices)} samples ({len(val_indices)/total_samples*100:.1f}%)")
        print(f"   Test set: {len(test_indices)} samples ({len(test_indices)/total_samples*100:.1f}%)")

        return split_config

    def load_data_split(self):
        """Load existing data split configuration"""

        if not os.path.exists(self.data_split_config):
            return None

        with open(self.data_split_config, 'r') as f:
            split_config = json.load(f)

        print(f"ðŸ“‚ Loaded existing data split configuration:")
        print(f"   Training set: {split_config['train_size']} samples")
        print(f"   Validation set: {split_config['val_size']} samples")
        print(f"   Test set: {split_config['test_size']} samples")
        print(f"   Created: {split_config['created_timestamp']}")

        return split_config

    def apply_data_split(self, data, split_config):
        """Apply data split configuration to create train/val/test sets"""

        train_indices = np.array(split_config['train_indices'])
        val_indices = np.array(split_config['val_indices'])
        test_indices = np.array(split_config['test_indices'])

        # Create data splits
        splits = {
            'train': {
                'genus': data['genus'][train_indices],
                'species': data['species'][train_indices],
                'env': data['env'][train_indices],
                'density_log': data['density_log'][train_indices]
            },
            'val': {
                'genus': data['genus'][val_indices],
                'species': data['species'][val_indices],
                'env': data['env'][val_indices],
                'density_log': data['density_log'][val_indices]
            },
            'test': {
                'genus': data['genus'][test_indices],
                'species': data['species'][test_indices],
                'env': data['env'][test_indices],
                'density_log': data['density_log'][test_indices]
            }
        }

        return splits

    def create_improved_model(self, num_genera, num_species):
        """åˆ›å»ºæ”¹è¿›çš„æ¨¡åž‹æž¶æž„"""
        
        # è¾“å…¥å±‚
        genus_input = Input(shape=(1,), name='genus_input')
        species_input = Input(shape=(1,), name='species_input')
        env_input = Input(shape=(2,), name='env_input')  # pH, temperature
        
        # åµŒå…¥å±‚ - æ›´å¤§çš„åµŒå…¥ç»´åº¦
        genus_embedding = Embedding(num_genera, 16, name='genus_embedding')(genus_input)
        species_embedding = Embedding(num_species, 16, name='species_embedding')(species_input)
        
        # å±•å¹³åµŒå…¥
        genus_flat = Flatten()(genus_embedding)
        species_flat = Flatten()(species_embedding)
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        merged = Concatenate()([genus_flat, species_flat, env_input])
        
        # æ›´æ·±çš„ç½‘ç»œç»“æž„ï¼ŒåŠ å…¥æ‰¹å½’ä¸€åŒ–å’Œdropout
        x = Dense(256, activation='relu')(merged)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)
        
        x = Dense(512, activation='relu')(x)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)
        
        x = Dense(256, activation='relu')(x)
        x = BatchNormalization()(x)
        x = Dropout(0.2)(x)
        
        x = Dense(128, activation='relu')(x)
        x = BatchNormalization()(x)
        x = Dropout(0.2)(x)
        
        # è¾“å‡ºå±‚ - 24ä¸ªæ—¶é—´ç‚¹çš„å¯†åº¦é¢„æµ‹
        # ä½¿ç”¨sigmoidæ¿€æ´»ç¡®ä¿è¾“å‡ºåœ¨åˆç†èŒƒå›´å†…ï¼Œç„¶åŽç¼©æ”¾åˆ°log10èŒƒå›´
        output = Dense(24, activation='sigmoid', name='growth_curve')(x)
        # å°†sigmoidè¾“å‡º(0-1)æ˜ å°„åˆ°åˆç†çš„log10èŒƒå›´(3-9)ï¼Œå¯¹åº”1e3åˆ°1e9 CFU/mL
        output = tf.keras.layers.Lambda(lambda x: x * 6.0 + 3.0)(output)
        
        model = Model(inputs=[genus_input, species_input, env_input], outputs=output)
        
        return model
    
    def prepare_data(self, csv_file):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        
        print(f"ðŸ“‚ åŠ è½½æ•°æ®: {csv_file}")
        df = pd.read_csv(csv_file)
        
        # è§£æžç»†èŒåç§°
        bacteria_names = df.iloc[:, 0].str.split(' ', expand=True)
        genera = bacteria_names[0]
        species = bacteria_names[1]
        
        # ç¼–ç ç»†èŒä¿¡æ¯
        genus_encoded = self.genus_encoder.fit_transform(genera)
        species_encoded = self.species_encoder.fit_transform(species)
        
        # çŽ¯å¢ƒç‰¹å¾
        env_features = df[['ph', 'temperature']].values
        env_features_scaled = self.scaler.fit_transform(env_features)
        
        # å¯†åº¦æ•°æ® - è½¬æ¢ä¸ºlog10
        density_data = df.iloc[:, 3:27].values  # 24ä¸ªæ—¶é—´ç‚¹
        # ç¡®ä¿æ²¡æœ‰é›¶å€¼æˆ–è´Ÿå€¼
        density_data = np.maximum(density_data, 1.0)
        density_log = np.log10(density_data)
        
        print(f"ðŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   æ ·æœ¬æ•°: {len(df)}")
        print(f"   ç»†èŒå±ž: {len(self.genus_encoder.classes_)}")
        print(f"   ç»†èŒç§: {len(self.species_encoder.classes_)}")
        print(f"   å¯†åº¦èŒƒå›´: {np.min(density_data):.2e} - {np.max(density_data):.2e} CFU/mL")
        print(f"   Log10èŒƒå›´: {np.min(density_log):.2f} - {np.max(density_log):.2f}")
        
        return {
            'genus': genus_encoded,
            'species': species_encoded,
            'env': env_features_scaled,
            'density_log': density_log
        }
    
    def train_model(self, csv_file, epochs=100, batch_size=32, force_new_split=False):
        """è®­ç»ƒæ”¹è¿›çš„æ¨¡åž‹"""
        
        # å‡†å¤‡æ•°æ®
        data = self.prepare_data(csv_file)
        
        # åˆ›å»ºæ¨¡åž‹
        num_genera = len(self.genus_encoder.classes_)
        num_species = len(self.species_encoder.classes_)
        self.model = self.create_improved_model(num_genera, num_species)
        
        # ç¼–è¯‘æ¨¡åž‹ - ä½¿ç”¨è‡ªå®šä¹‰æŸå¤±å‡½æ•°
        self.model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss=self.biological_continuity_loss,
            metrics=['mse']
        )
        
        print(f"\nðŸ—ï¸  æ¨¡åž‹æž¶æž„:")
        self.model.summary()
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ•°æ®åˆ’åˆ†é…ç½®
        split_config = self.load_data_split()

        if split_config is None or force_new_split:
            print(f"ðŸ”„ Creating new data split...")
            split_config = self.create_data_split(data)
        else:
            print(f"ðŸ“‚ Using existing data split configuration")

        # åº”ç”¨æ•°æ®åˆ’åˆ†
        splits = self.apply_data_split(data, split_config)

        # å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®
        X_train = {
            'genus_input': splits['train']['genus'],
            'species_input': splits['train']['species'],
            'env_input': splits['train']['env']
        }
        y_train = splits['train']['density_log']

        X_val = {
            'genus_input': splits['val']['genus'],
            'species_input': splits['val']['species'],
            'env_input': splits['val']['env']
        }
        y_val = splits['val']['density_log']

        print(f"\nðŸ“Š Final data split for training:")
        print(f"   Training set: {len(y_train)} samples ({len(y_train)/len(data['genus'])*100:.1f}%)")
        print(f"   Validation set: {len(y_val)} samples ({len(y_val)/len(data['genus'])*100:.1f}%)")
        print(f"   Test set: {split_config['test_size']} samples ({split_config['test_size']/len(data['genus'])*100:.1f}%) - RESERVED")
        
        # å›žè°ƒå‡½æ•°
        callbacks = [
            EarlyStopping(patience=15, restore_best_weights=True, monitor='val_loss'),
            ReduceLROnPlateau(patience=8, factor=0.5, min_lr=1e-6, monitor='val_loss')
        ]
        
        # è®­ç»ƒæ¨¡åž‹
        print(f"\nðŸš€ å¼€å§‹è®­ç»ƒ...")
        print(f"âš ï¸  æ³¨æ„: æµ‹è¯•é›†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®Œå…¨ä¸å¯è§ï¼Œä»…ç”¨äºŽæœ€ç»ˆè¯„ä¼°")
        history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        return history
    
    def save_model(self, model_path):
        """ä¿å­˜æ¨¡åž‹"""
        self.model.save(model_path)
        print(f"ðŸ’¾ æ¨¡åž‹å·²ä¿å­˜åˆ°: {model_path}")

if __name__ == "__main__":
    # åˆ›å»ºæ”¹è¿›çš„æ¨¡åž‹è®­ç»ƒå™¨
    trainer = ImprovedBacteriaGrowthModel()
    
    # ä½¿ç”¨æ¸…ç†åŽçš„æ•°æ®è®­ç»ƒæ¨¡åž‹
    print("ðŸ§  è®­ç»ƒæ”¹è¿›çš„ç»†èŒç”Ÿé•¿é¢„æµ‹æ¨¡åž‹...")
    history = trainer.train_model("bacteria_24h_cleaned_dataset.csv", epochs=150, batch_size=64)
    
    # ä¿å­˜æ”¹è¿›çš„æ¨¡åž‹
    trainer.save_model("bacteria_growth_model.h5")
    
    print("âœ… æ”¹è¿›æ¨¡åž‹è®­ç»ƒå®Œæˆï¼")
